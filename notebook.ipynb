{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Function1'> None [<minitorch.autodiff.Variable object at 0x127395b80>, <minitorch.autodiff.Variable object at 0x127395b80>] 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minitorch\n",
    "import pytest\n",
    "from minitorch import History\n",
    "\n",
    "class FunctionBase:\n",
    "    \"\"\"\n",
    "    A function that can act on :class:`Variable` arguments to\n",
    "    produce a :class:`Variable` output, while tracking the internal history.\n",
    "\n",
    "    Call by :func:`FunctionBase.apply`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def variable(raw, history):\n",
    "        # Implement by children class.\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *vals):\n",
    "        \"\"\"\n",
    "        Apply is called by the user to run the Function.\n",
    "        Internally it does three things:\n",
    "\n",
    "        a) Creates a Context for the function call.\n",
    "        b) Calls forward to run the function.\n",
    "        c) Attaches the Context to the History of the new variable.\n",
    "\n",
    "        There is a bit of internal complexity in our implementation\n",
    "        to handle both scalars and tensors.\n",
    "\n",
    "        Args:\n",
    "            vals (list of Variables or constants) : The arguments to forward\n",
    "\n",
    "        Returns:\n",
    "            `Variable` : The new variable produced\n",
    "\n",
    "        \"\"\"\n",
    "        # Go through the variables to see if any needs grad.\n",
    "        raw_vals = []\n",
    "        need_grad = False\n",
    "        for v in vals:\n",
    "            if isinstance(v, Variable):\n",
    "                if v.history is not None:\n",
    "                    need_grad = True\n",
    "                v.used += 1\n",
    "                raw_vals.append(v.get_data())\n",
    "            else:\n",
    "                raw_vals.append(v)\n",
    "\n",
    "        # Create the context.\n",
    "        ctx = Context(not need_grad)\n",
    "\n",
    "        # Call forward with the variables.\n",
    "        c = cls.forward(ctx, *raw_vals)\n",
    "        assert isinstance(c, cls.data_type), \"Expected return typ %s got %s\" % (\n",
    "            cls.data_type,\n",
    "            type(c),\n",
    "        )\n",
    "\n",
    "        # Create a new variable from the result with a new history.\n",
    "        back = None\n",
    "        if need_grad:\n",
    "            back = History(cls, ctx, vals)\n",
    "        return cls.variable(cls.data(c), back)\n",
    "\n",
    "    @classmethod\n",
    "    def chain_rule(cls, ctx, inputs, d_output):\n",
    "        \"\"\"\n",
    "        Implement the derivative chain-rule.\n",
    "\n",
    "        Args:\n",
    "            ctx (:class:`Context`) : The context from running forward\n",
    "            inputs (list of args) : The args that were passed to :func:`FunctionBase.apply` (e.g. :math:`x, y`)\n",
    "            d_output (number) : The `d_output` value in the chain rule.\n",
    "\n",
    "        Returns:\n",
    "            list of (`Variable`, number) : A list of non-constant variables with their derivatives\n",
    "            (see `is_constant` to remove unneeded variables)\n",
    "\n",
    "        \"\"\"\n",
    "        # Tip: Note when implementing this function that\n",
    "        # cls.backward may return either a value or a tuple.\n",
    "\n",
    "        print(cls, ctx, inputs, d_output)\n",
    "        \n",
    "        return []\n",
    "\n",
    "\n",
    "class ScalarFunction(FunctionBase):\n",
    "    \"\"\"\n",
    "    A wrapper for a mathematical function that processes and produces\n",
    "    Scalar variables.\n",
    "\n",
    "    This is a static class and is never instantiated. We use `class`\n",
    "    here to group together the `forward` and `backward` code.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        r\"\"\"\n",
    "        Forward call, compute :math:`f(x_0 \\ldots x_{n-1})`.\n",
    "\n",
    "        Args:\n",
    "            ctx (:class:`Context`): A container object to save\n",
    "                                    any information that may be needed\n",
    "                                    for the call to backward.\n",
    "            *inputs (list of floats): n-float values :math:`x_0 \\ldots x_{n-1}`.\n",
    "\n",
    "        Should return float the computation of the function :math:`f`.\n",
    "        \"\"\"\n",
    "        pass  # pragma: no cover\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_out):\n",
    "        r\"\"\"\n",
    "        Backward call, computes :math:`f'_{x_i}(x_0 \\ldots x_{n-1}) \\times d_{out}`.\n",
    "\n",
    "        Args:\n",
    "            ctx (Context): A container object holding any information saved during in the corresponding `forward` call.\n",
    "            d_out (float): :math:`d_out` term in the chain rule.\n",
    "\n",
    "        Should return the computation of the derivative function\n",
    "        :math:`f'_{x_i}` for each input :math:`x_i` times `d_out`.\n",
    "\n",
    "        \"\"\"\n",
    "        pass  # pragma: no cover\n",
    "\n",
    "    # Checks.\n",
    "    variable = minitorch.scalar.Scalar\n",
    "    data_type = float\n",
    "\n",
    "    @staticmethod\n",
    "    def data(a):\n",
    "        return a\n",
    "\n",
    "\n",
    "# ## Task 1.3 - Tests for the autodifferentiation machinery.\n",
    "\n",
    "# Simple sanity check and debugging tests.\n",
    "\n",
    "\n",
    "class Function1(ScalarFunction):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        \":math:`f(x, y) = x + y + 10`\"\n",
    "        return x + y + 10\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        \"Derivatives are :math:`f'_x(x, y) = 1` and :math:`f'_y(x, y) = 1`\"\n",
    "        return d_output, d_output\n",
    "\n",
    "\n",
    "class Function2(ScalarFunction):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        \":math:`f(x, y) = x \\timex y + x`\"\n",
    "        ctx.save_for_backward(x, y)\n",
    "        return x * y + x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        \"Derivatives are :math:`f'_x(x, y) = y + 1` and :math:`f'_y(x, y) = x`\"\n",
    "        x, y = ctx.saved_values\n",
    "        return d_output * (y + 1), d_output * x\n",
    "\n",
    "\n",
    "# Checks for the chain rule function.\n",
    "\n",
    "\n",
    "# @pytest.mark.task1_3\n",
    "# def test_chain_rule1():\n",
    "#     \"Check that constants are ignored.\"\n",
    "#     constant = minitorch.Variable(None)\n",
    "#     back = Function1.chain_rule(ctx=None, inputs=[constant, constant], d_output=5)\n",
    "#     assert len(list(back)) == 0\n",
    "\n",
    "\n",
    "# @pytest.mark.task1_3\n",
    "# def test_chain_rule2():\n",
    "#     \"Check that constrants are ignored and variables get derivatives.\"\n",
    "#     var = minitorch.Variable(History())\n",
    "#     constant = minitorch.Variable(None)\n",
    "#     back = Function1.chain_rule(ctx=None, inputs=[var, constant], d_output=5)\n",
    "#     back = list(back)\n",
    "#     assert len(back) == 1\n",
    "#     variable, deriv = back[0]\n",
    "#     assert variable.name == var.name\n",
    "#     assert deriv == 5\n",
    "\n",
    "\n",
    "# @pytest.mark.task1_3\n",
    "# def test_chain_rule3():\n",
    "#     \"Check that constrants are ignored and variables get derivatives.\"\n",
    "#     constant = 10\n",
    "#     var = minitorch.Scalar(5)\n",
    "\n",
    "#     ctx = minitorch.Context()\n",
    "#     Function2.forward(ctx, constant, var.data)\n",
    "\n",
    "#     back = Function2.chain_rule(ctx=ctx, inputs=[constant, var], d_output=5)\n",
    "#     back = list(back)\n",
    "#     assert len(back) == 1\n",
    "#     variable, deriv = back[0]\n",
    "#     assert variable.name == var.name\n",
    "#     assert deriv == 5 * 10\n",
    "\n",
    "# @pytest.mark.task1_3\n",
    "# def test_chain_rule4():\n",
    "#     var1 = minitorch.Scalar(5)\n",
    "#     var2 = minitorch.Scalar(10)\n",
    "\n",
    "#     ctx = minitorch.Context()\n",
    "#     Function2.forward(ctx, var1.data, var2.data)\n",
    "\n",
    "#     back = Function2.chain_rule(ctx=ctx, inputs=[var1, var2], d_output=5)\n",
    "#     back = list(back)\n",
    "#     assert len(back) == 2\n",
    "#     variable, deriv = back[0]\n",
    "#     assert variable.name == var1.name\n",
    "#     assert deriv == 5 * (10 + 1)\n",
    "#     variable, deriv = back[1]\n",
    "#     assert variable.name == var2.name\n",
    "#     assert deriv == 5 * 5\n",
    "\n",
    "constant = minitorch.Variable(None)\n",
    "back = Function1.chain_rule(ctx=None, inputs=[constant, constant], d_output=5)\n",
    "# assert len(list(back)) == 0\n",
    "\n",
    "back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(func, vars_list, eps=1e-4):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func(*vars_list)\n",
    "\n",
    "    for idx in range(len(vars_list)):\n",
    "        tweaked_vars = vars_list[:]\n",
    "        tweaked_vars[idx] += eps\n",
    "        tweaked_func_eval = func(*tweaked_vars)\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    \n",
    "    return partial_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x*x*y + y + 2\n",
    "\n",
    "def df(x, y):\n",
    "    return gradients(f, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.000400000048216, 10.000000000047748]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.000400000048216, 10.000000000047748)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dfdx(x, y):\n",
    "    return gradients(f, [x, y])[0]\n",
    "\n",
    "def dfdy(x, y):\n",
    "    return gradients(f, [x, y])[1]\n",
    "\n",
    "dfdx(3, 4), dfdy(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.999999951380232, 6.000099261882497],\n",
       " [6.000099261882497, -1.4210854715202004e-06]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def d2f(x, y):\n",
    "    return [gradients(dfdx, [x, y]), gradients(dfdy, [x, y])]\n",
    "\n",
    "d2f(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class Var:\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class BinaryOperator:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.a} + {self.b}\"\n",
    "\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.a} * {self.b}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = 3\n",
    "y.value = 4\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.46761419430053]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sin\n",
    "\n",
    "def z(x):\n",
    "    return sin(x ** 2)\n",
    "\n",
    "gradients(z, [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Const.gradient = lambda self, var: Const(0)\n",
    "Var.gradient = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.gradient = lambda self, var: Add(self.a.gradient(var), self.b.gradient(var))\n",
    "Mul.gradient = lambda self, var: Add(Mul(self.a, self.b.gradient(var)), Mul(self.a.gradient(var), self.b))\n",
    "\n",
    "x = Var(name=\"x\", init_value=3.)\n",
    "y = Var(name=\"y\", init_value=4.)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2)))\n",
    "\n",
    "dfdx = f.gradient(x)\n",
    "dfdy = f.gradient(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 10.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx.evaluate(), dfdy.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0, 6.0, 6.0, 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[g.evaluate() for g in [dfdx.gradient(x), dfdx.gradient(y), dfdy.gradient(x), dfdy.gradient(y)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(): # f(x,y) = x²y + y + 2\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def evaluate(self):\n",
    "        print('const eval', self.value)\n",
    "        return self.value\n",
    "    \n",
    "    def backpropagate(self, gradient):\n",
    "        print('const back', gradient)\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var: # f(x,y) = x²y + y + 2\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "        print('var eval', self.value)\n",
    "        return self.value\n",
    "\n",
    "    def backpropagate(self, gradient):\n",
    "        print('var back', self.gradient, gradient)\n",
    "        self.gradient += gradient\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(): # f(x,y) = x²y + y + 2\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "class Add(BinaryOperator): # f(x,y) = x²y + y + 2\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        print('add eval', self.a.evaluate(), self.b.evaluate())\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        print('add back', self.a, self.b, gradient)\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator): # f(x,y) = x²y + y + 2\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        print('mul eval', self.a.evaluate(), self.b.evaluate())\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        print('mul back', self.a, self.b, gradient)\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"{} * {}\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\", init_value=3)\n",
    "y = Var(\"y\", init_value=4)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2)))\n",
    "\n",
    "result = f.evaluate()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add back x * x * y y + 2 1.0\n",
      "mul back x * x y 1.0\n",
      "mul back x x 4.0\n",
      "var back 0 12.0\n",
      "var back 12.0 12.0\n",
      "var back 0 9.0\n",
      "add back y 2 1.0\n",
      "var back 9.0 1.0\n",
      "const back 1.0\n"
     ]
    }
   ],
   "source": [
    "f.backpropagate(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x * x * y + y + 2\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i = 0,  s = 4,  product = 8,  divisor = 2.0  ordinal = 1.0\n",
      " i = 1,  s = 1,  product = 2,  divisor = 2.0  ordinal = 1.0\n",
      " i = 2,  s = 2,  product = 2,  divisor = 1.0  ordinal = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "ordinal = 1  # (1, 0)\n",
    "shape = (4, 1, 2)\n",
    "# array = np.array([[[1, 1, 1]], [[1, 1, 1]], [[0, 1, 1]], [[0, 1, 1]]])\n",
    "out_index = []\n",
    "\n",
    "for i, s in enumerate(shape):\n",
    "    product = reduce(mul, shape[i:], 1)\n",
    "    divisor = product / s\n",
    "    index = int(ordinal // divisor)\n",
    "    \n",
    "    ordinal -= divisor * index\n",
    "    out_index.append(index)\n",
    "\n",
    "    print(f\"{ i = }, { s = }, { product = }, { divisor = } { ordinal = }\")\n",
    "\n",
    "out_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ordinal = 3\n",
    "a, b, c = array.shape # (4, 1, 3)\n",
    "print(ordinal // a)\n",
    "\n",
    "ordinal /=  a\n",
    "ordinal # 3\n",
    "\n",
    "print(12 // b)\n",
    "ordinal /= b\n",
    "ordinal # 3\n",
    "\n",
    "c - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 // 2 -> 1\n",
    "# 1 // 3 -> 0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2edef401dfbc4ce3548fd8a806c4cf8a803ba27855b675b0bad7f2383e27583e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('minitorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
